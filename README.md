# ProtoExplore
A backend service with an API using protocol buffers and gRPC.

## Structure
This project has three components split across two containers:

#### Database
A PostgreSQL database which exists in its own container. The definitions for the database's
tables are defined in `init.sql` in the root of this repo. Once running the database can be
called from the port `50051`. On startup the database is empty, the generation of data is
handled by the `server` component.

#### Server
The server is written in Go (located in `server/`) and uses gRPC with protocol buffers for the API. The protocol buffer
serialisation and gRPC implementation are generated from the service definition `explore-service.proto`.
You can see the generated source code in `explore/`. The server is built to `bin/server` along with the CLI.

Once running the server executes the following operations in order before waiting for requests:
- Attempts to connect to the database and will retry 10 times before exiting. This is so the database 
has time to accept the incoming connections
- The server then generates all users and decisions and inserts them into the database. I settled on 50 users
which generates an addition 2000 decisions between them all
- Once user data is generated the server writes a `client.id` file to the `bin/` directory, I explain
the reason for this below
- The server then starts the gRPC server and waits for requests from a client

#### CLI
The CLI is written in Go (located in `cli/`) and like the server uses gRPC with protocol buffers for communication.
The CLI is built to `bin/cli` along with the server and is intended to be run within the container it is built in while
the server is running.

Once running the CLI executes the following operations in order before becoming interactive:
- The CLI attempts to read the `client.id` file in the `bin/` directory generated by the server, I explain
  the reason for this below
- The CLI then starts the gRPC client which targets the server
- The CLI then becomes interactive from the command line and is used to interact with the server
as a user

## Building and Running
Requires protoc with go and gRPC plugins and Docker with access to docker-compose.
```bash
./run.sh
```

Once complete two containers should be running and the logs of both being seen in the terminal. If
everything went okay you should see this output somewhere near the bottom:

`client-server  | [SERVER]: ... listening on 127.0.0.1:50051`

Once you can see the server is listening, running the CLI will work as follows in another terminal:
```bash
docker exec -it client-server ./bin/cli
```

Once you want to clean up everything use this:
```bash
docker-compose down
```

Something to note is that the user the CLI is acting as does not change at all once the server has begun. Meaning if you want
to restart you need to rebuild with docker-compose again. This is due to the `client.id` file issue. This also means that if you
restart the server it will generate all data, with no assumption that there could be existing data in the database.

## Using the CLI
This is the main menu from the CLI and provides a list of options to use. Enter the number of the option
you wish to run. 
```
--- Protoexplore ---
1) See everyone who liked you
2) See likes from people you haven't yet
3) Get your total likes
4) Match with people who liked you
5) Exit
>
```
- 1: See a list of all users who have liked you, this output is paginated so continuing to press enter will
give you more results
- 2: See a list of all users who have liked you, but you haven't liked them back. Either you have already
passed or not made a decision yet
- 3: See the total number of like you have across all other users
- 4: Start matching with all users who have liked you. You can like or pass each of them, some of these users you
have already passed on and some others you have not seen before

## Decisions on how it was built
- I picked a CLI as it kept things simple and meant I could do the client in go as well as the server
- I needed to split the NewLikedList into two separate queries to the database as I was not able to find
a way to do it in a single query
- Due to this now being two separate queries it means filtering it to a single list of users is done on the
server when the request is made. This for sure is not how you should do this and ended up being O(n^2). So
don't push to this to production
- I was able to find a solution to paginating the liked list for all likes. But due to my NewLikeList
endpoint needing two queries to the database it meant it was difficult to do and I didn't find a way to do it.
- My assumption was that I was not to edit the API spec at all and due to it not containing any way to
pass the user id of the client from server I decided to hack it together by writing it to a file once
the data was generated. The file is `bin/client.id` and is read by the CLI on startup and that id is then used
for all following requests. While I am not happy with how this worked out I wasn't sure how to pass a 
user id to the client with it being either manually done or getting around my assumption of the API being only
what is defined in the proto file
- Due to the CLI and server needing to pass the client id file to each other it means the easiest way to get things
working was having them operate in the same container. While I guess this isn't "correct" I don't really mind it
as it is more important what they are doing and how instead
- I choose to use uuids for each user_id. I picked this due to them not being in order and random. The downside to this
however is when pagination is done I use the user_id as the token and therefore they are sorted by their uuids. Of course at
my scale that is fine but in a large system that might become a problem, and also it means writes in between requests can not
give a true result as new rows can be inserted in any order

